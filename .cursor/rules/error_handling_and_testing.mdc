# Error Handling and Testing Standards

## Description
This rule establishes error handling and testing standards for the Living Truth Engine project, ensuring proper error reporting and comprehensive testing.

## ðŸš« **No Fallback Mechanisms**

### **Fail-Fast Principle**
- **âŒ NO FALLBACKS**: Never create placeholder files or silent fallbacks
- **âœ… THROW ERRORS**: If something fails, throw a clear error immediately
- **âœ… CLEAR MESSAGING**: Error messages should explain exactly what failed and why
- **âœ… PROPER EXCEPTIONS**: Use appropriate exception types (RuntimeError, ValueError, etc.)

### **Examples of Bad Error Handling**
```python
# âŒ BAD - Silent fallback
try:
    result = some_operation()
except Exception as e:
    # Creates placeholder file instead of throwing error
    create_placeholder_file()
    return "Warning: Operation failed but created placeholder"

# âŒ BAD - Generic error handling
try:
    result = some_operation()
except Exception as e:
    return f"Error: {e}"  # Too generic
```

### **Examples of Good Error Handling**
```python
# âœ… GOOD - Clear error with context
try:
    result = some_operation()
except FileNotFoundError as e:
    raise RuntimeError(f"Required file not found: {e}")
except ConnectionError as e:
    raise RuntimeError(f"Service connection failed: {e}")
except Exception as e:
    raise RuntimeError(f"Unexpected error in some_operation: {e}")

# âœ… GOOD - Specific error types
def generate_audio(text: str) -> str:
    try:
        voice = PiperVoice.load("en_US-lessac-medium.onnx")
        voice.synthesize(text, output_path)
        return f"âœ… Audio generated successfully: {output_path}"
    except FileNotFoundError as e:
        raise RuntimeError(f"Piper TTS voice model not found: {e}")
    except Exception as e:
        raise RuntimeError(f"TTS generation failed: {e}")
```

## ðŸ§ª **Functional Testing Standards**

### **Test Categories**
1. **Service Functionality**: Test actual service capabilities, not just health checks
2. **Data Processing**: Verify real data processing and analysis
3. **Integration**: Test service interactions and dependencies
4. **Error Conditions**: Test proper error handling and reporting
5. **Performance**: Verify response times and resource usage

### **Functional Test Requirements**
```python
# âœ… GOOD - Tests actual functionality
def test_audio_generation_functionality(self):
    """Test audio generation can create actual audio files"""
    try:
        result = self.engine.generate_audio("Test text")
        
        # Verify actual audio file was created
        audio_files = list(Path("data/outputs/audio").glob("*.wav"))
        if not audio_files:
            return False
            
        latest_audio = max(audio_files, key=lambda x: x.stat().st_mtime)
        file_size = latest_audio.stat().st_size
        
        # Real audio files should be substantial
        if file_size < 1000:
            return False
            
        return True
    except Exception as e:
        # If audio generation fails, it should throw an error
        logger.error(f"Audio generation test failed: {e}")
        return False

# âŒ BAD - Only tests health checks
def test_audio_generation(self):
    """Test audio generation health check"""
    response = requests.get("http://localhost:8050/health")
    return response.status_code == 200  # This doesn't test actual audio generation
```

### **Test Coverage Requirements**
- **100% Core Functionality**: All main features must have functional tests
- **Error Path Testing**: Test error conditions and proper error handling
- **Integration Testing**: Test service interactions and dependencies
- **Performance Testing**: Verify response times and resource usage
- **Data Validation**: Verify output data quality and structure

## ðŸ“Š **Testing Metrics**

### **Functional Test Results**
- **âœ… 6/7 Tests Passing**: Core functionality working
- **âŒ 1/7 Tests Failing**: Audio generation needs piper-tts models
- **ðŸŽ¯ 85% Coverage**: Most functionality verified

### **Current Test Categories**
1. **âœ… Langflow Workflow**: API accessible, health checks pass
2. **âœ… Dashboard Visualization**: Interface loads, data validation works
3. **âœ… LM Studio Models**: Model availability and API access verified
4. **âŒ Audio Generation**: Fails due to missing piper-tts voice models
5. **âœ… Transcript Analysis**: Real data processing verified
6. **âœ… Visualization Generation**: Network graph creation verified
7. **âœ… MCP Server Tools**: Core MCP functionality verified

## ðŸ”§ **Error Handling Implementation**

### **Current Error Handling**
- **âœ… Audio Generation**: Throws RuntimeError when piper-tts models missing
- **âœ… Langflow Workflow**: Throws NotImplementedError for unimplemented features
- **âœ… MCP Server**: Proper error propagation and logging
- **âœ… Database Operations**: Clear error messages for connection issues

### **Error Handling Checklist**
- [ ] **No silent failures** - All errors are logged and reported
- [ ] **Clear error messages** - Errors explain what failed and why
- [ ] **Proper exception types** - Use appropriate exception classes
- [ ] **Error context** - Include relevant context in error messages
- [ ] **No fallback mechanisms** - Don't create placeholder data
- [ ] **Fail fast** - Throw errors immediately when operations fail

## ðŸŽ¯ **Best Practices**

### **Error Handling**
1. **Be Specific**: Use specific exception types and clear error messages
2. **Include Context**: Provide relevant context about what failed
3. **Log Errors**: Always log errors with appropriate detail
4. **Fail Fast**: Don't continue with invalid state
5. **No Placeholders**: Never create placeholder or dummy data

### **Testing**
1. **Test Functionality**: Test what the service actually does, not just if it's running
2. **Test Error Paths**: Verify proper error handling and reporting
3. **Test Real Data**: Use real data and verify actual processing
4. **Test Integration**: Verify service interactions work correctly
5. **Test Performance**: Ensure response times meet requirements

### **Quality Assurance**
1. **Run Functional Tests**: Execute comprehensive functional tests regularly
2. **Monitor Error Rates**: Track and analyze error patterns
3. **Validate Output**: Verify output data quality and structure
4. **Performance Monitoring**: Monitor response times and resource usage
5. **Documentation**: Keep error handling and testing documentation current

---

**Follow these standards to ensure robust error handling and comprehensive testing throughout the Living Truth Engine project.**
description:
globs:
alwaysApply: false
---
